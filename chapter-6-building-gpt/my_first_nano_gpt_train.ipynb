{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "eQ7YpSBwt5C1"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "#hyperparameters\n",
    "batch_size = 64 #how many independent sequences will be process in parallel?\n",
    "block_size = 256  #what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 300\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "#---------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# Download the text file (assuming you're running this in an environment litok_embke a Jupyter notebook or Colab)\n",
    "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "\n",
    "# Read the entire text file into a single string\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "# Get all the unique characters that occur in the text\n",
    "chars = sorted(list(set(text)))\n",
    "\n",
    "# Determine the size of the vocabulary (number of unique characters)\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# Create a mapping from characters to integers (stoi: string-to-integer)\n",
    "# Example: {'\\n': 0, ' ': 1, '!': 2, ...}\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "\n",
    "# Create a mapping from integers back to characters (itos: integer-to-string)\n",
    "# This is the inverse of stoi. Example: {0: '\\n', 1: ' ', 2: '!', ...}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "# Define an encoder function: takes a string and outputs a list of integers\n",
    "# by looking up the integer representation for each character.\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "\n",
    "# Define a decoder function: takes a list of integers and outputs a string\n",
    "# by joining the corresponding characters together.\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "# Convert the entire text dataset into a PyTorch tensor of integers\n",
    "import torch # This line is implied or needs to be added if not present\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "# Determine the split point (90% for training, 10% for validation)\n",
    "n = int(0.9*len(data)) # 90% first will be train, rest val\n",
    "\n",
    "# Create the training dataset (first 90% of the integer-encoded data)\n",
    "train_data = data[:n]\n",
    "\n",
    "# Create the validation dataset (the remaining 10%)\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "\n",
    "    # Selects the appropriate dataset (train_data or val_data) based on the 'split' argument\n",
    "    data = train_data if split == 'train' else val_data\n",
    "\n",
    "    # Generates a set of random starting indices (ix) for the chunks of text.\n",
    "    # block_size is the length of the sequence (e.g., 8 characters), batch_size is how many sequences to get.\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "\n",
    "    # Stacks the input sequences (x).\n",
    "    # For each index 'i' in 'ix', it takes a slice of 'data' of length 'block_size'.\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "\n",
    "    # Stacks the target sequences (y).\n",
    "    # The target for a given input sequence is the same sequence shifted one position to the right.\n",
    "    # This means the model is trained to predict the character at position i+1 based on the character at position i.\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "\n",
    "    # Moves the input and target tensors to the specified device (e.g., 'cuda' for GPU, or 'cpu').\n",
    "    x, y = x.to(device), y.to(device)\n",
    "\n",
    "    return x, y\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "# Decorator that tells PyTorch not to calculate gradients for this function.\n",
    "# This saves memory and computation time as we are only evaluating, not training.\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "\n",
    "    # Sets the model to evaluation mode. This disables features like Dropout and BatchNorm updates.\n",
    "    model.eval()\n",
    "\n",
    "    # Loop over both the training and validation splits\n",
    "    for split in ['train', 'val']:\n",
    "\n",
    "        # Initializes a tensor to store the losses for a specified number of evaluation iterations.\n",
    "        losses = torch.zeros(eval_iters)\n",
    "\n",
    "        # Runs the evaluation loop\n",
    "        for k in range(eval_iters):\n",
    "            # Gets a batch of data for the current split\n",
    "            X, Y = get_batch(split)\n",
    "\n",
    "            # Performs a forward pass through the model\n",
    "            logits, loss = model(X, Y)\n",
    "\n",
    "            # Stores the loss value\n",
    "            losses[k] = loss.item()\n",
    "\n",
    "        # Calculates the average loss across all evaluation iterations for the current split\n",
    "        out[split] = losses.mean().item()\n",
    "\n",
    "    # Sets the model back to training mode (enabling features like Dropout/BatchNorm)\n",
    "    model.train()\n",
    "\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x) # (B,T,C\n",
    "        q = self.query(x) # (B,T,C\n",
    "        #compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T) #scaled attention\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T) decoder block\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        #perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T,\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #each token directly reads off the logits for the next token from the lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) #final layer norm\n",
    "        # self.blocks = nn.Sequential(\n",
    "        #     Block(n_embd, n_head=4),\n",
    "        #     Block(n_embd, n_head=4),\n",
    "        #     Block(n_embd, n_head=4),\n",
    "        #     nn.LayerNorm(n_embd),\n",
    "        # )\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        #idx and targets are both (B, T) tensor of integers\n",
    "        tok_emb =  self.token_embedding_table(idx) #(B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) #(T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        #idx is (B, T) array of indices in the current target\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            #get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            #focus only on the last time step\n",
    "            logits = logits[:, -1, :] #becomes (B, C)\n",
    "            #apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=1) # (B, C)\n",
    "            #sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            #append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ml9mtjYSud0p",
    "outputId": "7ae40cb2-2ac4-4053-c07d-d5e7c9c7a2e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.4753, val loss 4.4709\n",
      "step 300: train loss 2.3898, val loss 2.4239\n",
      "step 600: train loss 1.9617, val loss 2.0524\n",
      "step 900: train loss 1.7100, val loss 1.8558\n",
      "step 1200: train loss 1.5732, val loss 1.7534\n",
      "step 1500: train loss 1.4913, val loss 1.6879\n",
      "step 1800: train loss 1.4212, val loss 1.6233\n",
      "step 2100: train loss 1.3659, val loss 1.5909\n",
      "step 2400: train loss 1.3260, val loss 1.5626\n",
      "step 2700: train loss 1.2899, val loss 1.5414\n",
      "step 3000: train loss 1.2592, val loss 1.5242\n",
      "step 3300: train loss 1.2304, val loss 1.5128\n",
      "step 3600: train loss 1.2115, val loss 1.5039\n",
      "step 3900: train loss 1.1850, val loss 1.4943\n",
      "step 4200: train loss 1.1618, val loss 1.4911\n",
      "step 4500: train loss 1.1362, val loss 1.4918\n",
      "step 4800: train loss 1.1187, val loss 1.4912\n"
     ]
    }
   ],
   "source": [
    "#--------Training---------\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "\n",
    "#PyTorch Optimizer\n",
    "optmizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "\n",
    "    #every once in while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "\n",
    "    #sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    #evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optmizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optmizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2lfn83A4t_uB",
    "outputId": "14b5f2cb-6e93-452a-934e-eb2a7b232c87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to shakespeare_model.pth\n"
     ]
    }
   ],
   "source": [
    "#saving the model - weights only\n",
    "torch.save(model.state_dict(), \"my-first-gpt.pth\")\n",
    "print(\"Model saved to my-first-gpt.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MAo3lWHKuEjC",
    "outputId": "7191bbc0-8126-43b1-876e-2f72ab9c7c76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BUCKINGHAM:\n",
      "Good part, as ends me, to come.\n",
      "But my Carspidel. O Lord God, that 'tis word\n",
      "After me worth interr'd true in this doverty men;\n",
      "Upon every my slaving with side,\n",
      "And his swellips sleet legs in him!\n",
      "Who is he hairs: come to desire the got\n",
      "Soon sweet hence of it. I can tell the provour\n",
      "Thou their bears their doing-fallier's, or rotagion.\n",
      "\n",
      "ISABELLA:\n",
      "Alas, parder'd, alas! not sir.\n",
      "\n",
      "ABULET:\n",
      "But pridim tell'd me, leave it with this dissensed,\n",
      "For Ensixth a disdoing title: the poor petures\n",
      "no\n"
     ]
    }
   ],
   "source": [
    "#generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aMfELDoluHXk",
    "outputId": "03b3ebc4-b04a-4cf6-fea2-3ff5ff123e33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of generated text:\n",
      "\n",
      "\n",
      "I ark.\n",
      "\n",
      "AUTOLYCUS:\n",
      "Age it is not order?\n",
      "\n",
      "CORIOLANUS:\n",
      "And thou mad?\n",
      "\n",
      "COMIOLANUS:\n",
      "I thank to go.\n",
      "\n",
      "Third Servingman:\n",
      "Away! a unducteous is but of life in Vienna\n",
      "tack for your honour own from all age! The day to-morrow,\n",
      "shear, was with already in his graced counted coloursed by.\n",
      "\n",
      "VOLUMNIA:\n",
      "I would have would.\n",
      "Now, very wedding do what most\n",
      "Chow from me, that first thou conquains touches, they\n",
      "save shepherded in stay 'shaldier'd.'\n",
      "\n",
      "CORIOLANUS:\n",
      "Despated you, ray, sir, I did; and enjoice now twice\n",
      "And\n",
      "\n",
      "Full generated text saved to shakespeare_output.txt\n"
     ]
    }
   ],
   "source": [
    "#generate text and save to a file\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "generated_indices = model.generate(context, max_new_tokens=2000)[0].tolist()\n",
    "generated_text = decode(generated_indices)\n",
    "\n",
    "# Print a preview\n",
    "print(\"\\nSample of generated text:\\n\")\n",
    "print(generated_text[:500])  # first 500 characters\n",
    "\n",
    "# Save full text\n",
    "with open(\"shakespeare_output.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(generated_text)\n",
    "\n",
    "print(\"\\nFull generated text saved to shakespeare_output.txt\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
