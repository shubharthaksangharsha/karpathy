{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "JHxedkDPOqj9"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from dataclasses import dataclass\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import inspect\n",
        "\n",
        "#--------------------------------------------------------\n",
        "class CausalSelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        # key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd) #bias ON\n",
        "        # output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
        "        # regularization\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        # not really a 'bias', more of a mask, but following the OpenAI/HF naming though\n",
        "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "                                .view(1, 1, config.block_size, config.block_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "        #calculate query, key, values for all heads in batch and move head forward to be the batch\n",
        "        # nh is \"number of heads\", hs is \"head size\", and C (number of channels) = nh * hs\n",
        "        # e.g. in GPT-2 (124M), n_head=12, hs=64, so nh*hs=C=768 channels in the Transformer\n",
        "        qkv = self.c_attn(x)\n",
        "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        # attention (materializes the large (T,T) matrix for all the queries and keys)\n",
        "\n",
        "        #flash attention v1 as T4 supports v1 only\n",
        "\n",
        "        # att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        # att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
        "        # att = F.softmax(att, dim=-1)\n",
        "        # y = att @ v  # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
        "\n",
        "\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "        # output projection\n",
        "        y = self.c_proj(y)\n",
        "        return y\n",
        "\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
        "        self.gelu = nn.GELU(approximate='tanh')\n",
        "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
        "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = 1024 # max sequence length\n",
        "    vocab_size: int = 50257 # number of tokens: 50,000 BPE merges + 256 bytes tokens + 1 <|endoftext|> token\n",
        "    n_layer: int = 12 # number of layers\n",
        "    n_head: int = 12 # number of heads\n",
        "    n_embd: int = 768 # embedding dimension\n",
        "\n",
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = nn.LayerNorm(config.n_embd),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "        # weight sharing scheme\n",
        "        self.transformer.wte.weight = self.lm_head.weight\n",
        "\n",
        "        # init params\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            std = 0.02\n",
        "            if hasattr(module, \"NANOGPT_SCALE_INIT\"):\n",
        "                std *= 2 * (self.config.n_layer) ** -0.5\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        # idx is of shape (B, T)\n",
        "        B, T = idx.size()\n",
        "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size\"\n",
        "        # forward the token and position embeddings\n",
        "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device).unsqueeze(0) # shape (T)\n",
        "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (T, n_embd)\n",
        "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (B, T, n_embd)\n",
        "        x = tok_emb + pos_emb\n",
        "        # forward the blocks of the transformer\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        # forward the final layernorm and the classifer\n",
        "        x = self.transformer.ln_f(x)\n",
        "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "        return logits, loss\n",
        "\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, model_type):\n",
        "        \"\"\"Loads pretrained GPT-2 model weights from huggingface\"\"\"\n",
        "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
        "        from transformers import GPT2LMHeadModel\n",
        "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
        "\n",
        "        # n_layer, n_head and n_embd are determined from model_type\n",
        "        config_args = {\n",
        "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
        "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
        "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
        "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
        "        }[model_type]\n",
        "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
        "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
        "        # create a from-scratch initialized minGPT model\n",
        "        config = GPTConfig(**config_args)\n",
        "        model = GPT(config)\n",
        "        sd = model.state_dict()\n",
        "        sd_keys = sd.keys()\n",
        "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
        "\n",
        "        # init a huggingface/transformers model\n",
        "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
        "        sd_hf = model_hf.state_dict()\n",
        "\n",
        "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
        "        sd_keys_hf = sd_hf.keys()\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
        "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
        "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
        "        # this means that we have to transpose these weights when we import them\n",
        "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
        "        for k in sd_keys_hf:\n",
        "            if any(k.endswith(w) for w in transposed):\n",
        "                # special treatment for the Conv1D weights we need to transpose\n",
        "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k].t())\n",
        "            else:\n",
        "                # vanilla copy over the other parameters\n",
        "                assert sd_hf[k].shape == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k])\n",
        "\n",
        "        return model\n",
        "\n",
        "    def configure_optimizers(self, weight_decay, learning_rate, device_type):\n",
        "        # start with all of the candidate parameters (that require grad)\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
        "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
        "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
        "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
        "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
        "        optim_groups = [\n",
        "            {'params': decay_params, 'weight_decay': weight_decay},\n",
        "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
        "        ]\n",
        "        num_decay_params = sum(p.numel() for p in decay_params)\n",
        "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
        "        # if master_process:\n",
        "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
        "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
        "        # Create AdamW optimizer and use the fused version if it is available\n",
        "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
        "        use_fused = fused_available and device_type == \"cuda\"\n",
        "        # if master_process:\n",
        "        print(f\"using fused AdamW: {use_fused}\")\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=(0.9, 0.95), eps=1e-8, fused=use_fused)\n",
        "        return optimizer\n",
        "# -----------------------------------------------------------------------------\n",
        "import tiktoken\n",
        "\n",
        "\n",
        "class DataLoaderLite:\n",
        "    def __init__(self, B, T):\n",
        "        self.B = B\n",
        "        self.T = T\n",
        "\n",
        "        # at init load tokens form disk and store them in memory\n",
        "        with open('input.txt', 'r') as f:\n",
        "            text = f.read()\n",
        "        enc = tiktoken.get_encoding('gpt2')\n",
        "        tokens = enc.encode(text)\n",
        "        self.tokens = torch.tensor(tokens)\n",
        "        print(f\"loaded {len(tokens)} tokens\")\n",
        "        print(f\"1 epoch = {len(self.tokens) // (B * T)} batches\")\n",
        "\n",
        "        #state\n",
        "        self.current_position = 0\n",
        "\n",
        "    def next_batch(self):\n",
        "        B, T = self.B, self.T\n",
        "        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n",
        "        x = (buf[:-1]).view(B, T) # inputs\n",
        "        y = (buf[1:]).view(B, T) # outputs\n",
        "        # advance the position in the tensor\n",
        "        self.current_position += B * T\n",
        "        # if loading the next batch would be out of bounds, reset\n",
        "        if self.current_position + (B * T + 1) > len(self.tokens):\n",
        "            self.current_position = 0\n",
        "        return x, y\n",
        "\n",
        "# -----------------------------------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradient Accumulation"
      ],
      "metadata": {
        "id": "bWB2W3Nt1Fdy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# attempt to autodetect the device\n",
        "import time\n",
        "torch.backends.cuda.matmul.allow_tf32 = True  # no effect on T4, safe to enable\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "torch.set_float32_matmul_precision(\"high\")\n",
        "\n",
        "\n",
        "device = \"cpu\"\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "print(f\"using device: {device}\")\n",
        "# device = \"cpu\" #OVERRIDE\n",
        "# max_length = 30\n",
        "# num_return_sequences = 5\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(1337)\n",
        "\n",
        "total_batch_size = 524288 # 2**19, ~0.5M, in number of tokens\n",
        "B = 8 # micro batch size\n",
        "T = 1024 # sequence length\n",
        "assert total_batch_size % (B * T) == 0, \"make sure total_batch_size is divisible by B * T\"\n",
        "grad_accum_steps = total_batch_size // (B * T)\n",
        "print(f\"total desired batch size: {total_batch_size}\")\n",
        "print(f\"calculated gradient accumulation steps: {grad_accum_steps}\")\n",
        "\n",
        "train_loader = DataLoaderLite(B=8, T=1024)\n",
        "\n",
        "\n",
        "# get the logits\n",
        "model = GPT(GPTConfig(vocab_size=50304))\n",
        "# model = GPT.from_pretrained(\"gpt2\")\n",
        "model.to(device)\n",
        "\n",
        "#compile the model\n",
        "model = torch.compile(model)\n",
        "\n",
        "max_lr = 6e-4\n",
        "min_lr = max_lr * 0.1\n",
        "warmup_steps = 10\n",
        "max_steps = 50\n",
        "def get_lr(it):\n",
        "  # 1) linear warmup for warmup_iters steps\n",
        "  if it < warmup_steps:\n",
        "    return max_lr * (it+1) / warmup_steps\n",
        "  # 2) if it > lr_decay_iters, return min learning rate\n",
        "  if it >= max_steps:\n",
        "    return min_lr\n",
        "  # 3) in between, use consine decay down to min learning rate\n",
        "  decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n",
        "  assert 0 <= decay_ratio <= 1\n",
        "  coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges at 1 and goes to 0\n",
        "  return min_lr + coeff * (max_lr - min_lr)\n",
        "\n",
        "#optimize!\n",
        "from torch import amp\n",
        "\n",
        "scaler = amp.GradScaler(device=\"cuda\")  # NEW: enable mixed precision\n",
        "\n",
        "optimizer = model.configure_optimizers(weight_decay=0.1, learning_rate=6e-4, device_type=device)\n",
        "# optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, betas=(0.9, 0.95), eps=1e-8)\n",
        "\n",
        "for step in range(max_steps):\n",
        "    t0 = time.time()\n",
        "    optimizer.zero_grad()\n",
        "    loss_accum = 0.0\n",
        "\n",
        "    #determine and set the learning rate for this iteration\n",
        "    lr = get_lr(step)\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "    for micro_step in range(grad_accum_steps):\n",
        "      x, y = train_loader.next_batch()\n",
        "      x, y = x.to(device), y.to(device)\n",
        "\n",
        "      # FP16 forward + loss (autocast enables mixed precision on T4)\n",
        "      with amp.autocast(\"cuda\"):\n",
        "          logits, loss = model(x, y)\n",
        "\n",
        "      # we have to scale the loss to account for gradient accumulation,\n",
        "      # because the gradients just add on each successive backward().\n",
        "      # addition of gradients corresponds to a SUM in the objective, but\n",
        "      # instead of a SUM we want MEAN. Scale the loss here so it comes out right.\n",
        "      loss = loss / grad_accum_steps # scale the loss\n",
        "      loss_accum += loss.item()\n",
        "\n",
        "\n",
        "      # backward pass (scaled for safe gradients in FP16)\n",
        "      scaler.scale(loss).backward()\n",
        "\n",
        "    #unscale before clipping\n",
        "    scaler.unscale_(optimizer)\n",
        "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "\n",
        "    # update weights\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "\n",
        "    torch.cuda.synchronize() # wait for GPU to finish work\n",
        "    t1 = time.time()\n",
        "\n",
        "    dt = (t1 - t0) # time difference\n",
        "    ms = dt * 1000 #ms\n",
        "    tokens_processed = train_loader.B * train_loader.T * grad_accum_steps\n",
        "    tokens_per_sec = tokens_processed / dt\n",
        "    print(\n",
        "        f\" step {step:4d} | loss: {loss_accum:.6f} | lr: {lr:.4e} | \"\n",
        "        f\"norm: {norm:.4f} | dt: {ms:.2f}ms | tok/sec: {tokens_per_sec:.2f}\"\n",
        "     )\n",
        "\n",
        "\n",
        "import sys; sys.exit(0)\n",
        "\n",
        "\n",
        "#prefix tokens\n",
        "import tiktoken\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "tokens = enc.encode(\"Hello, I'm a language model,\")\n",
        "tokens = torch.tensor(tokens, dtype=torch.long) #(8, )\n",
        "tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1) #(5, 8)\n",
        "x = tokens.to(device)\n",
        "\n",
        "\n",
        "# generate! right now x is (B, T) where B = 5, T = 8\n",
        "# set the seed is 42 (my fav what is life :P)\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "while x.size(1) < max_length:\n",
        "    # forward the model to get the logits\n",
        "    with torch.no_grad():\n",
        "        logits = model(x) # (B, T, vocab_size)\n",
        "        # take the logits at the last position\n",
        "        logits = logits[:, -1, :] # (B, vocab_size)\n",
        "        # get the probabilities\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        # do top-k sampling of 50 (huggingface pipeline default)\n",
        "        # topk_probs here becomes (5, 50), topk_indices is  (5, 50)\n",
        "        topk_probs, topk_indices = torch.topk(probs, k=50, dim=-1)\n",
        "        # select a token from the top-k probabilities\n",
        "        ix = torch.multinomial(topk_probs, num_samples=1) # (B, 1)\n",
        "        # gather the corresponding indices\n",
        "        xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
        "        # append to the sequnce\n",
        "        x = torch.cat((x, xcol), dim=1)\n",
        "\n",
        "# print the generated text\n",
        "for i in range(num_return_sequences):\n",
        "    tokens = x[i, :max_length].tolist()\n",
        "    decoded = enc.decode(tokens)\n",
        "    print(\">\", decoded)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8L7B-jbe1KNN",
        "outputId": "b3ddd88b-48c7-4b66-fe92-8eb7265293e5"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using device: cuda\n",
            "total desired batch size: 524288\n",
            "calculated gradient accumulation steps: 64\n",
            "loaded 338025 tokens\n",
            "1 epoch = 41 batches\n",
            "num decayed parameter tensors: 50, with 124,354,560 parameters\n",
            "num non-decayed parameter tensors: 98, with 121,344 parameters\n",
            "using fused AdamW: True\n",
            " step    0 | loss: 10.961176 | lr: 6.0000e-05 | norm: 13.7307 | dt: 27031.35ms | tok/sec: 19395.55\n",
            " step    1 | loss: 9.763216 | lr: 1.2000e-04 | norm: 6.1150 | dt: 28921.67ms | tok/sec: 18127.86\n",
            " step    2 | loss: 9.339524 | lr: 1.8000e-04 | norm: 5.6734 | dt: 28643.70ms | tok/sec: 18303.78\n",
            " step    3 | loss: 9.667659 | lr: 2.4000e-04 | norm: 5.8280 | dt: 27450.20ms | tok/sec: 19099.61\n",
            " step    4 | loss: 9.066256 | lr: 3.0000e-04 | norm: 3.8648 | dt: 28097.71ms | tok/sec: 18659.46\n",
            " step    5 | loss: 8.615701 | lr: 3.6000e-04 | norm: 3.0571 | dt: 27388.44ms | tok/sec: 19142.68\n",
            " step    6 | loss: 8.372741 | lr: 4.2000e-04 | norm: 2.8869 | dt: 27243.26ms | tok/sec: 19244.69\n",
            " step    7 | loss: 8.053147 | lr: 4.8000e-04 | norm: 2.2701 | dt: 27200.21ms | tok/sec: 19275.15\n",
            " step    8 | loss: 7.710977 | lr: 5.4000e-04 | norm: 2.0495 | dt: 27145.67ms | tok/sec: 19313.87\n",
            " step    9 | loss: 7.390267 | lr: 6.0000e-04 | norm: 2.2355 | dt: 27075.25ms | tok/sec: 19364.11\n",
            " step   10 | loss: 7.063183 | lr: 6.0000e-04 | norm: 2.0324 | dt: 26964.68ms | tok/sec: 19443.51\n",
            " step   11 | loss: 6.819265 | lr: 5.9917e-04 | norm: 1.3125 | dt: 26952.62ms | tok/sec: 19452.21\n",
            " step   12 | loss: 6.597966 | lr: 5.9668e-04 | norm: 1.1940 | dt: 26966.40ms | tok/sec: 19442.27\n",
            " step   13 | loss: 6.464733 | lr: 5.9254e-04 | norm: 1.0048 | dt: 26967.26ms | tok/sec: 19441.65\n",
            " step   14 | loss: 6.339369 | lr: 5.8679e-04 | norm: 0.9506 | dt: 27040.08ms | tok/sec: 19389.29\n",
            " step   15 | loss: 6.264463 | lr: 5.7945e-04 | norm: 0.6162 | dt: 27134.43ms | tok/sec: 19321.88\n",
            " step   16 | loss: 6.293945 | lr: 5.7057e-04 | norm: 2.5238 | dt: 27297.77ms | tok/sec: 19206.26\n",
            " step   17 | loss: 6.243146 | lr: 5.6021e-04 | norm: 0.7600 | dt: 27019.84ms | tok/sec: 19403.81\n",
            " step   18 | loss: 6.269480 | lr: 5.4843e-04 | norm: 2.1996 | dt: 26932.54ms | tok/sec: 19466.71\n",
            " step   19 | loss: 6.228308 | lr: 5.3531e-04 | norm: 1.1866 | dt: 26958.50ms | tok/sec: 19447.96\n",
            " step   20 | loss: 6.228663 | lr: 5.2092e-04 | norm: 0.7330 | dt: 27001.34ms | tok/sec: 19417.11\n",
            " step   21 | loss: 6.167573 | lr: 5.0535e-04 | norm: 1.0121 | dt: 27152.62ms | tok/sec: 19308.93\n",
            " step   22 | loss: 6.132241 | lr: 4.8870e-04 | norm: 0.6871 | dt: 27050.85ms | tok/sec: 19381.57\n",
            " step   23 | loss: 6.095122 | lr: 4.7107e-04 | norm: 0.4755 | dt: 27041.61ms | tok/sec: 19388.19\n",
            " step   24 | loss: 6.071612 | lr: 4.5258e-04 | norm: 0.6548 | dt: 27104.76ms | tok/sec: 19343.02\n",
            " step   25 | loss: 6.053391 | lr: 4.3332e-04 | norm: 0.3527 | dt: 26969.57ms | tok/sec: 19439.98\n",
            " step   26 | loss: 6.032718 | lr: 4.1343e-04 | norm: 0.4576 | dt: 26836.22ms | tok/sec: 19536.58\n",
            " step   27 | loss: 6.034856 | lr: 3.9303e-04 | norm: 0.5953 | dt: 26789.99ms | tok/sec: 19570.29\n",
            " step   28 | loss: 5.994111 | lr: 3.7224e-04 | norm: 0.6223 | dt: 26871.90ms | tok/sec: 19510.64\n",
            " step   29 | loss: 6.007313 | lr: 3.5118e-04 | norm: 0.4423 | dt: 26830.86ms | tok/sec: 19540.48\n",
            " step   30 | loss: 5.974462 | lr: 3.3000e-04 | norm: 0.3085 | dt: 26865.08ms | tok/sec: 19515.59\n",
            " step   31 | loss: 5.961621 | lr: 3.0882e-04 | norm: 0.3782 | dt: 26881.97ms | tok/sec: 19503.33\n",
            " step   32 | loss: 5.950231 | lr: 2.8776e-04 | norm: 0.3049 | dt: 26867.53ms | tok/sec: 19513.82\n",
            " step   33 | loss: 5.936590 | lr: 2.6697e-04 | norm: 0.2539 | dt: 26868.60ms | tok/sec: 19513.04\n",
            " step   34 | loss: 5.928627 | lr: 2.4657e-04 | norm: 0.4386 | dt: 26904.16ms | tok/sec: 19487.25\n",
            " step   35 | loss: 5.922826 | lr: 2.2668e-04 | norm: 0.3830 | dt: 26989.92ms | tok/sec: 19425.33\n",
            " step   36 | loss: 5.941931 | lr: 2.0742e-04 | norm: 0.8011 | dt: 26735.70ms | tok/sec: 19610.03\n",
            " step   37 | loss: 5.898977 | lr: 1.8893e-04 | norm: 0.3393 | dt: 26855.51ms | tok/sec: 19522.55\n",
            " step   38 | loss: 5.909421 | lr: 1.7130e-04 | norm: 0.3286 | dt: 26971.96ms | tok/sec: 19438.26\n",
            " step   39 | loss: 5.899879 | lr: 1.5465e-04 | norm: 0.4633 | dt: 27040.86ms | tok/sec: 19388.73\n",
            " step   40 | loss: 5.888337 | lr: 1.3908e-04 | norm: 0.2993 | dt: 27004.67ms | tok/sec: 19414.72\n",
            " step   41 | loss: 5.891007 | lr: 1.2469e-04 | norm: 0.2101 | dt: 26941.64ms | tok/sec: 19460.14\n",
            " step   42 | loss: 5.881911 | lr: 1.1157e-04 | norm: 0.3038 | dt: 26897.58ms | tok/sec: 19492.01\n",
            " step   43 | loss: 5.890781 | lr: 9.9787e-05 | norm: 0.3020 | dt: 26859.85ms | tok/sec: 19519.40\n",
            " step   44 | loss: 5.869231 | lr: 8.9428e-05 | norm: 0.2509 | dt: 27013.47ms | tok/sec: 19408.39\n",
            " step   45 | loss: 5.891137 | lr: 8.0553e-05 | norm: 0.1727 | dt: 27097.21ms | tok/sec: 19348.41\n",
            " step   46 | loss: 5.871865 | lr: 7.3215e-05 | norm: 0.1826 | dt: 27206.70ms | tok/sec: 19270.55\n",
            " step   47 | loss: 5.874089 | lr: 6.7460e-05 | norm: 0.2184 | dt: 27204.83ms | tok/sec: 19271.87\n",
            " step   48 | loss: 5.869370 | lr: 6.3324e-05 | norm: 0.2595 | dt: 27152.67ms | tok/sec: 19308.89\n",
            " step   49 | loss: 5.869613 | lr: 6.0832e-05 | norm: 0.2285 | dt: 27197.01ms | tok/sec: 19277.42\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "0",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# saving the model\n",
        "torch.save(model.state_dict(), \"gpt2_only_weights.pt\")\n"
      ],
      "metadata": {
        "id": "JH3vNsIMA755"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Batch Size Schedule + weight decay + FusedAdamW"
      ],
      "metadata": {
        "id": "R9ewtHurvH-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# attempt to autodetect the device\n",
        "import time\n",
        "torch.backends.cuda.matmul.allow_tf32 = True  # no effect on T4, safe to enable\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "torch.set_float32_matmul_precision(\"high\")\n",
        "\n",
        "\n",
        "device = \"cpu\"\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "print(f\"using device: {device}\")\n",
        "# device = \"cpu\" #OVERRIDE\n",
        "# max_length = 30\n",
        "# num_return_sequences = 5\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(1337)\n",
        "\n",
        "train_loader = DataLoaderLite(B=8, T=1024)\n",
        "\n",
        "\n",
        "# get the logits\n",
        "model = GPT(GPTConfig(vocab_size=50304))\n",
        "# model = GPT.from_pretrained(\"gpt2\")\n",
        "model.to(device)\n",
        "\n",
        "#compile the model\n",
        "model = torch.compile(model)\n",
        "\n",
        "max_lr = 6e-4\n",
        "min_lr = max_lr * 0.1\n",
        "warmup_steps = 10\n",
        "max_steps = 50\n",
        "def get_lr(it):\n",
        "  # 1) linear warmup for warmup_iters steps\n",
        "  if it < warmup_steps:\n",
        "    return max_lr * (it+1) / warmup_steps\n",
        "  # 2) if it > lr_decay_iters, return min learning rate\n",
        "  if it >= max_steps:\n",
        "    return min_lr\n",
        "  # 3) in between, use consine decay down to min learning rate\n",
        "  decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n",
        "  assert 0 <= decay_ratio <= 1\n",
        "  coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges at 1 and goes to 0\n",
        "  return min_lr + coeff * (max_lr - min_lr)\n",
        "\n",
        "#optimize!\n",
        "from torch import amp\n",
        "\n",
        "scaler = amp.GradScaler(device=\"cuda\")  # NEW: enable mixed precision\n",
        "\n",
        "optimizer = model.configure_optimizers(weight_decay=0.1, learning_rate=6e-4, device_type=device)\n",
        "# optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, betas=(0.9, 0.95), eps=1e-8)\n",
        "\n",
        "for step in range(50):\n",
        "    t0 = time.time()\n",
        "\n",
        "    x, y = train_loader.next_batch()\n",
        "    x, y = x.to(device), y.to(device)\n",
        "\n",
        "    #determine and set the learning rate for this iteration\n",
        "    lr = get_lr(step)\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # FP16 forward + loss (autocast enables mixed precision on T4)\n",
        "    with amp.autocast(\"cuda\"):\n",
        "        logits, loss = model(x, y)\n",
        "\n",
        "    # backward pass (scaled for safe gradients in FP16)\n",
        "    scaler.scale(loss).backward()\n",
        "\n",
        "    #unscale before clipping\n",
        "    scaler.unscale_(optimizer)\n",
        "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "\n",
        "    # update weights\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    t1 = time.time()\n",
        "\n",
        "    dt = (t1 - t0) * 100  # time difference in milliseconds\n",
        "    tokens_per_sec = (train_loader.B * train_loader.T) / (t1 - t0)\n",
        "    print(f\" step {step:4d} | loss: {loss.item():.6f} | lr: {lr:.4e} | norm: {norm:.4f} | dt: {dt:.2f}ms | tok/sec: {tokens_per_sec:.2f}\")\n",
        "\n",
        "import sys; sys.exit(0)\n",
        "\n",
        "\n",
        "#prefix tokens\n",
        "import tiktoken\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "tokens = enc.encode(\"Hello, I'm a language model,\")\n",
        "tokens = torch.tensor(tokens, dtype=torch.long) #(8, )\n",
        "tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1) #(5, 8)\n",
        "x = tokens.to(device)\n",
        "\n",
        "\n",
        "# generate! right now x is (B, T) where B = 5, T = 8\n",
        "# set the seed is 42 (my fav what is life :P)\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "while x.size(1) < max_length:\n",
        "    # forward the model to get the logits\n",
        "    with torch.no_grad():\n",
        "        logits = model(x) # (B, T, vocab_size)\n",
        "        # take the logits at the last position\n",
        "        logits = logits[:, -1, :] # (B, vocab_size)\n",
        "        # get the probabilities\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        # do top-k sampling of 50 (huggingface pipeline default)\n",
        "        # topk_probs here becomes (5, 50), topk_indices is  (5, 50)\n",
        "        topk_probs, topk_indices = torch.topk(probs, k=50, dim=-1)\n",
        "        # select a token from the top-k probabilities\n",
        "        ix = torch.multinomial(topk_probs, num_samples=1) # (B, 1)\n",
        "        # gather the corresponding indices\n",
        "        xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
        "        # append to the sequnce\n",
        "        x = torch.cat((x, xcol), dim=1)\n",
        "\n",
        "# print the generated text\n",
        "for i in range(num_return_sequences):\n",
        "    tokens = x[i, :max_length].tolist()\n",
        "    decoded = enc.decode(tokens)\n",
        "    print(\">\", decoded)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "atL4OhYyvPHz",
        "outputId": "fa82890f-2e15-474f-b8b7-f299bc4d5cc9"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using device: cuda\n",
            "loaded 338025 tokens\n",
            "1 epoch = 41 batches\n",
            "num decayed parameter tensors: 50, with 124,354,560 parameters\n",
            "num non-decayed parameter tensors: 98, with 121,344 parameters\n",
            "using fused AdamW: True\n",
            " step    0 | loss: 10.945843 | lr: 6.0000e-05 | norm: 15.1871 | dt: 188.48ms | tok/sec: 4346.26\n",
            " step    1 | loss: 9.764231 | lr: 1.2000e-04 | norm: 6.3446 | dt: 40.91ms | tok/sec: 20022.32\n",
            " step    2 | loss: 9.106415 | lr: 1.8000e-04 | norm: 5.3951 | dt: 42.73ms | tok/sec: 19173.17\n",
            " step    3 | loss: 9.450719 | lr: 2.4000e-04 | norm: 6.0236 | dt: 41.80ms | tok/sec: 19597.15\n",
            " step    4 | loss: 8.929743 | lr: 3.0000e-04 | norm: 4.0037 | dt: 40.82ms | tok/sec: 20066.88\n",
            " step    5 | loss: 8.684343 | lr: 3.6000e-04 | norm: 2.6439 | dt: 42.13ms | tok/sec: 19444.11\n",
            " step    6 | loss: 8.620053 | lr: 4.2000e-04 | norm: 3.5140 | dt: 41.00ms | tok/sec: 19979.91\n",
            " step    7 | loss: 8.186541 | lr: 4.8000e-04 | norm: 2.2540 | dt: 41.50ms | tok/sec: 19738.48\n",
            " step    8 | loss: 7.806085 | lr: 5.4000e-04 | norm: 2.5878 | dt: 40.94ms | tok/sec: 20010.44\n",
            " step    9 | loss: 7.618732 | lr: 6.0000e-04 | norm: 2.3645 | dt: 40.55ms | tok/sec: 20204.10\n",
            " step   10 | loss: 7.422826 | lr: 6.0000e-04 | norm: 1.9221 | dt: 41.97ms | tok/sec: 19516.88\n",
            " step   11 | loss: 7.159819 | lr: 5.9917e-04 | norm: 1.4761 | dt: 41.12ms | tok/sec: 19920.99\n",
            " step   12 | loss: 7.061849 | lr: 5.9668e-04 | norm: 1.5748 | dt: 41.25ms | tok/sec: 19859.57\n",
            " step   13 | loss: 6.826506 | lr: 5.9254e-04 | norm: 1.0301 | dt: 41.59ms | tok/sec: 19696.17\n",
            " step   14 | loss: 6.706924 | lr: 5.8679e-04 | norm: 1.2585 | dt: 41.52ms | tok/sec: 19728.83\n",
            " step   15 | loss: 6.509318 | lr: 5.7945e-04 | norm: 1.2370 | dt: 42.01ms | tok/sec: 19502.07\n",
            " step   16 | loss: 6.603495 | lr: 5.7057e-04 | norm: 0.9516 | dt: 41.81ms | tok/sec: 19591.15\n",
            " step   17 | loss: 6.725049 | lr: 5.6021e-04 | norm: 4.1668 | dt: 41.32ms | tok/sec: 19826.77\n",
            " step   18 | loss: 6.629426 | lr: 5.4843e-04 | norm: 1.7086 | dt: 42.53ms | tok/sec: 19261.61\n",
            " step   19 | loss: 6.412034 | lr: 5.3531e-04 | norm: 0.9616 | dt: 41.99ms | tok/sec: 19508.41\n",
            " step   20 | loss: 6.539159 | lr: 5.2092e-04 | norm: 1.2772 | dt: 41.47ms | tok/sec: 19754.96\n",
            " step   21 | loss: 6.339879 | lr: 5.0535e-04 | norm: 1.4443 | dt: 42.10ms | tok/sec: 19458.66\n",
            " step   22 | loss: 6.453913 | lr: 4.8870e-04 | norm: 0.9513 | dt: 42.01ms | tok/sec: 19498.45\n",
            " step   23 | loss: 6.289311 | lr: 4.7107e-04 | norm: 1.7444 | dt: 42.67ms | tok/sec: 19200.10\n",
            " step   24 | loss: 6.301044 | lr: 4.5258e-04 | norm: 1.0036 | dt: 42.28ms | tok/sec: 19377.29\n",
            " step   25 | loss: 6.341768 | lr: 4.3332e-04 | norm: 1.1000 | dt: 42.57ms | tok/sec: 19244.02\n",
            " step   26 | loss: 6.671055 | lr: 4.1343e-04 | norm: 1.2335 | dt: 42.70ms | tok/sec: 19185.12\n",
            " step   27 | loss: 6.514644 | lr: 3.9303e-04 | norm: 1.2106 | dt: 42.31ms | tok/sec: 19360.24\n",
            " step   28 | loss: 6.760203 | lr: 3.7224e-04 | norm: 1.1541 | dt: 42.80ms | tok/sec: 19140.67\n",
            " step   29 | loss: 6.516041 | lr: 3.5118e-04 | norm: 1.1974 | dt: 42.31ms | tok/sec: 19361.31\n",
            " step   30 | loss: 6.466401 | lr: 3.3000e-04 | norm: 1.1462 | dt: 42.76ms | tok/sec: 19156.56\n",
            " step   31 | loss: 6.457513 | lr: 3.0882e-04 | norm: 1.3819 | dt: 42.47ms | tok/sec: 19288.19\n",
            " step   32 | loss: 6.300709 | lr: 2.8776e-04 | norm: 1.1288 | dt: 42.17ms | tok/sec: 19426.46\n",
            " step   33 | loss: 6.623689 | lr: 2.6697e-04 | norm: 1.5057 | dt: 42.79ms | tok/sec: 19145.78\n",
            " step   34 | loss: 6.468144 | lr: 2.4657e-04 | norm: 1.1235 | dt: 42.17ms | tok/sec: 19425.24\n",
            " step   35 | loss: 6.379628 | lr: 2.2668e-04 | norm: 0.7454 | dt: 42.59ms | tok/sec: 19234.64\n",
            " step   36 | loss: 6.429451 | lr: 2.0742e-04 | norm: 0.9790 | dt: 42.45ms | tok/sec: 19295.94\n",
            " step   37 | loss: 6.443392 | lr: 1.8893e-04 | norm: 0.9406 | dt: 42.46ms | tok/sec: 19292.83\n",
            " step   38 | loss: 6.225785 | lr: 1.7130e-04 | norm: 0.8614 | dt: 42.77ms | tok/sec: 19152.65\n",
            " step   39 | loss: 6.291762 | lr: 1.5465e-04 | norm: 0.9891 | dt: 42.65ms | tok/sec: 19206.66\n",
            " step   40 | loss: 6.491573 | lr: 1.3908e-04 | norm: 1.1276 | dt: 42.74ms | tok/sec: 19165.51\n",
            " step   41 | loss: 6.323069 | lr: 1.2469e-04 | norm: 1.1561 | dt: 43.25ms | tok/sec: 18941.42\n",
            " step   42 | loss: 6.368238 | lr: 1.1157e-04 | norm: 1.2398 | dt: 41.76ms | tok/sec: 19614.58\n",
            " step   43 | loss: 6.108902 | lr: 9.9787e-05 | norm: 1.4835 | dt: 42.94ms | tok/sec: 19077.11\n",
            " step   44 | loss: 6.048880 | lr: 8.9428e-05 | norm: 1.0207 | dt: 42.04ms | tok/sec: 19486.77\n",
            " step   45 | loss: 6.117875 | lr: 8.0553e-05 | norm: 0.9015 | dt: 42.36ms | tok/sec: 19337.25\n",
            " step   46 | loss: 6.183800 | lr: 7.3215e-05 | norm: 0.7138 | dt: 42.18ms | tok/sec: 19420.69\n",
            " step   47 | loss: 6.063455 | lr: 6.7460e-05 | norm: 1.1370 | dt: 42.03ms | tok/sec: 19492.16\n",
            " step   48 | loss: 5.959907 | lr: 6.3324e-05 | norm: 0.8344 | dt: 42.47ms | tok/sec: 19289.19\n",
            " step   49 | loss: 5.851988 | lr: 6.0832e-05 | norm: 0.8786 | dt: 43.22ms | tok/sec: 18952.58\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "0",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding Learning Rate Schedular: Warmup + cosine decay (As instructed in GPT-3 Paper)"
      ],
      "metadata": {
        "id": "68605MGPrinQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# attempt to autodetect the device\n",
        "import time\n",
        "torch.backends.cuda.matmul.allow_tf32 = True  # no effect on T4, safe to enable\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "torch.set_float32_matmul_precision(\"high\")\n",
        "\n",
        "\n",
        "device = \"cpu\"\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "print(f\"using device: {device}\")\n",
        "# device = \"cpu\" #OVERRIDE\n",
        "# max_length = 30\n",
        "# num_return_sequences = 5\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(1337)\n",
        "\n",
        "train_loader = DataLoaderLite(B=8, T=1024)\n",
        "\n",
        "\n",
        "# get the logits\n",
        "model = GPT(GPTConfig(vocab_size=50304))\n",
        "# model = GPT.from_pretrained(\"gpt2\")\n",
        "model.to(device)\n",
        "\n",
        "#compile the model\n",
        "model = torch.compile(model)\n",
        "\n",
        "max_lr = 6e-4\n",
        "min_lr = max_lr * 0.1\n",
        "warmup_steps = 10\n",
        "max_steps = 50\n",
        "def get_lr(it):\n",
        "  # 1) linear warmup for warmup_iters steps\n",
        "  if it < warmup_steps:\n",
        "    return max_lr * (it+1) / warmup_steps\n",
        "  # 2) if it > lr_decay_iters, return min learning rate\n",
        "  if it >= max_steps:\n",
        "    return min_lr\n",
        "  # 3) in between, use consine decay down to min learning rate\n",
        "  decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n",
        "  assert 0 <= decay_ratio <= 1\n",
        "  coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges at 1 and goes to 0\n",
        "  return min_lr + coeff * (max_lr - min_lr)\n",
        "\n",
        "#optimize!\n",
        "from torch import amp\n",
        "\n",
        "scaler = amp.GradScaler(device=\"cuda\")  # NEW: enable mixed precision\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, betas=(0.9, 0.95), eps=1e-8)\n",
        "\n",
        "for step in range(50):\n",
        "    t0 = time.time()\n",
        "\n",
        "    x, y = train_loader.next_batch()\n",
        "    x, y = x.to(device), y.to(device)\n",
        "\n",
        "    #determine and set the learning rate for this iteration\n",
        "    lr = get_lr(step)\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # FP16 forward + loss (autocast enables mixed precision on T4)\n",
        "    with amp.autocast(\"cuda\"):\n",
        "        logits, loss = model(x, y)\n",
        "\n",
        "    # backward pass (scaled for safe gradients in FP16)\n",
        "    scaler.scale(loss).backward()\n",
        "\n",
        "    #unscale before clipping\n",
        "    scaler.unscale_(optimizer)\n",
        "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "\n",
        "    # update weights\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    t1 = time.time()\n",
        "\n",
        "    dt = (t1 - t0) * 100  # time difference in milliseconds\n",
        "    tokens_per_sec = (train_loader.B * train_loader.T) / (t1 - t0)\n",
        "    print(f\" step {step:4d} | loss: {loss.item():.6f} | lr: {lr:.4e} | norm: {norm:.4f} | dt: {dt:.2f}ms | tok/sec: {tokens_per_sec:.2f}\")\n",
        "\n",
        "import sys; sys.exit(0)\n",
        "\n",
        "\n",
        "#prefix tokens\n",
        "import tiktoken\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "tokens = enc.encode(\"Hello, I'm a language model,\")\n",
        "tokens = torch.tensor(tokens, dtype=torch.long) #(8, )\n",
        "tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1) #(5, 8)\n",
        "x = tokens.to(device)\n",
        "\n",
        "\n",
        "# generate! right now x is (B, T) where B = 5, T = 8\n",
        "# set the seed is 42 (my fav what is life :P)\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "while x.size(1) < max_length:\n",
        "    # forward the model to get the logits\n",
        "    with torch.no_grad():\n",
        "        logits = model(x) # (B, T, vocab_size)\n",
        "        # take the logits at the last position\n",
        "        logits = logits[:, -1, :] # (B, vocab_size)\n",
        "        # get the probabilities\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        # do top-k sampling of 50 (huggingface pipeline default)\n",
        "        # topk_probs here becomes (5, 50), topk_indices is  (5, 50)\n",
        "        topk_probs, topk_indices = torch.topk(probs, k=50, dim=-1)\n",
        "        # select a token from the top-k probabilities\n",
        "        ix = torch.multinomial(topk_probs, num_samples=1) # (B, 1)\n",
        "        # gather the corresponding indices\n",
        "        xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
        "        # append to the sequnce\n",
        "        x = torch.cat((x, xcol), dim=1)\n",
        "\n",
        "# print the generated text\n",
        "for i in range(num_return_sequences):\n",
        "    tokens = x[i, :max_length].tolist()\n",
        "    decoded = enc.decode(tokens)\n",
        "    print(\">\", decoded)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "AArwMZvzriSX",
        "outputId": "62123553-8417-4296-af68-653895c8a521"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using device: cuda\n",
            "loaded 338025 tokens\n",
            "1 epoch = 41 batches\n",
            " step    0 | loss: 10.945843 | lr: 6.0000e-05 | norm: 15.1871 | dt: 47.23ms | tok/sec: 17344.90\n",
            " step    1 | loss: 9.764231 | lr: 1.2000e-04 | norm: 6.3446 | dt: 42.49ms | tok/sec: 19277.70\n",
            " step    2 | loss: 9.106407 | lr: 1.8000e-04 | norm: 5.3955 | dt: 44.62ms | tok/sec: 18358.99\n",
            " step    3 | loss: 9.450704 | lr: 2.4000e-04 | norm: 6.0235 | dt: 42.35ms | tok/sec: 19343.18\n",
            " step    4 | loss: 8.929711 | lr: 3.0000e-04 | norm: 4.0037 | dt: 45.13ms | tok/sec: 18151.21\n",
            " step    5 | loss: 8.684285 | lr: 3.6000e-04 | norm: 2.6437 | dt: 43.62ms | tok/sec: 18779.22\n",
            " step    6 | loss: 8.620027 | lr: 4.2000e-04 | norm: 3.5158 | dt: 42.79ms | tok/sec: 19142.95\n",
            " step    7 | loss: 8.186493 | lr: 4.8000e-04 | norm: 2.2554 | dt: 43.34ms | tok/sec: 18899.58\n",
            " step    8 | loss: 7.806010 | lr: 5.4000e-04 | norm: 2.5879 | dt: 42.04ms | tok/sec: 19486.43\n",
            " step    9 | loss: 7.618584 | lr: 6.0000e-04 | norm: 2.3638 | dt: 43.83ms | tok/sec: 18692.40\n",
            " step   10 | loss: 7.422726 | lr: 6.0000e-04 | norm: 1.9220 | dt: 42.95ms | tok/sec: 19072.27\n",
            " step   11 | loss: 7.159669 | lr: 5.9917e-04 | norm: 1.4762 | dt: 42.90ms | tok/sec: 19093.43\n",
            " step   12 | loss: 7.061743 | lr: 5.9668e-04 | norm: 1.5759 | dt: 42.30ms | tok/sec: 19367.16\n",
            " step   13 | loss: 6.826441 | lr: 5.9254e-04 | norm: 1.0302 | dt: 42.58ms | tok/sec: 19240.72\n",
            " step   14 | loss: 6.706926 | lr: 5.8679e-04 | norm: 1.2583 | dt: 43.88ms | tok/sec: 18670.64\n",
            " step   15 | loss: 6.509363 | lr: 5.7945e-04 | norm: 1.2367 | dt: 43.38ms | tok/sec: 18885.63\n",
            " step   16 | loss: 6.603586 | lr: 5.7057e-04 | norm: 0.9518 | dt: 43.45ms | tok/sec: 18855.93\n",
            " step   17 | loss: 6.724458 | lr: 5.6021e-04 | norm: 4.8142 | dt: 43.95ms | tok/sec: 18640.79\n",
            " step   18 | loss: 6.634212 | lr: 5.4843e-04 | norm: 1.7980 | dt: 43.58ms | tok/sec: 18799.50\n",
            " step   19 | loss: 6.415525 | lr: 5.3531e-04 | norm: 0.9972 | dt: 43.47ms | tok/sec: 18846.27\n",
            " step   20 | loss: 6.543604 | lr: 5.2092e-04 | norm: 1.2974 | dt: 43.77ms | tok/sec: 18716.86\n",
            " step   21 | loss: 6.344531 | lr: 5.0535e-04 | norm: 1.5222 | dt: 43.48ms | tok/sec: 18839.62\n",
            " step   22 | loss: 6.456106 | lr: 4.8870e-04 | norm: 0.9394 | dt: 43.76ms | tok/sec: 18720.86\n",
            " step   23 | loss: 6.294328 | lr: 4.7107e-04 | norm: 1.0696 | dt: 43.68ms | tok/sec: 18754.77\n",
            " step   24 | loss: 6.289596 | lr: 4.5258e-04 | norm: 0.9837 | dt: 44.27ms | tok/sec: 18505.69\n",
            " step   25 | loss: 6.332116 | lr: 4.3332e-04 | norm: 0.8430 | dt: 44.13ms | tok/sec: 18563.20\n",
            " step   26 | loss: 6.659027 | lr: 4.1343e-04 | norm: 1.2850 | dt: 43.37ms | tok/sec: 18888.66\n",
            " step   27 | loss: 6.502038 | lr: 3.9303e-04 | norm: 1.4777 | dt: 43.96ms | tok/sec: 18633.86\n",
            " step   28 | loss: 6.748580 | lr: 3.7224e-04 | norm: 1.0845 | dt: 43.49ms | tok/sec: 18836.52\n",
            " step   29 | loss: 6.502364 | lr: 3.5118e-04 | norm: 1.1826 | dt: 43.62ms | tok/sec: 18780.88\n",
            " step   30 | loss: 6.455837 | lr: 3.3000e-04 | norm: 0.8612 | dt: 43.92ms | tok/sec: 18653.98\n",
            " step   31 | loss: 6.441134 | lr: 3.0882e-04 | norm: 1.0822 | dt: 43.29ms | tok/sec: 18925.56\n",
            " step   32 | loss: 6.286734 | lr: 2.8776e-04 | norm: 1.1562 | dt: 43.77ms | tok/sec: 18714.49\n",
            " step   33 | loss: 6.600675 | lr: 2.6697e-04 | norm: 1.5081 | dt: 43.50ms | tok/sec: 18831.09\n",
            " step   34 | loss: 6.442623 | lr: 2.4657e-04 | norm: 0.9773 | dt: 44.40ms | tok/sec: 18452.36\n",
            " step   35 | loss: 6.358432 | lr: 2.2668e-04 | norm: 0.8897 | dt: 44.09ms | tok/sec: 18581.93\n",
            " step   36 | loss: 6.402664 | lr: 2.0742e-04 | norm: 1.0542 | dt: 43.39ms | tok/sec: 18879.30\n",
            " step   37 | loss: 6.416093 | lr: 1.8893e-04 | norm: 0.9314 | dt: 44.27ms | tok/sec: 18505.07\n",
            " step   38 | loss: 6.196319 | lr: 1.7130e-04 | norm: 0.9502 | dt: 43.91ms | tok/sec: 18657.04\n",
            " step   39 | loss: 6.265736 | lr: 1.5465e-04 | norm: 0.9283 | dt: 43.89ms | tok/sec: 18663.97\n",
            " step   40 | loss: 6.467690 | lr: 1.3908e-04 | norm: 1.1362 | dt: 43.82ms | tok/sec: 18695.87\n",
            " step   41 | loss: 6.300958 | lr: 1.2469e-04 | norm: 0.9707 | dt: 44.41ms | tok/sec: 18444.89\n",
            " step   42 | loss: 6.349833 | lr: 1.1157e-04 | norm: 1.1803 | dt: 44.47ms | tok/sec: 18422.15\n",
            " step   43 | loss: 6.072339 | lr: 9.9787e-05 | norm: 1.2347 | dt: 44.40ms | tok/sec: 18448.48\n",
            " step   44 | loss: 6.016899 | lr: 8.9428e-05 | norm: 0.8740 | dt: 44.74ms | tok/sec: 18308.80\n",
            " step   45 | loss: 6.092099 | lr: 8.0553e-05 | norm: 0.9553 | dt: 44.39ms | tok/sec: 18456.10\n",
            " step   46 | loss: 6.161164 | lr: 7.3215e-05 | norm: 0.8047 | dt: 44.48ms | tok/sec: 18415.76\n",
            " step   47 | loss: 6.034010 | lr: 6.7460e-05 | norm: 1.2164 | dt: 44.50ms | tok/sec: 18409.15\n",
            " step   48 | loss: 5.930587 | lr: 6.3324e-05 | norm: 0.9367 | dt: 44.59ms | tok/sec: 18371.09\n",
            " step   49 | loss: 5.819634 | lr: 6.0832e-05 | norm: 1.0333 | dt: 44.46ms | tok/sec: 18423.91\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "0",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Added Gradient clipping (unscale before clipping) + Setting AdamW Hyperparameters as GPT-3 Paper"
      ],
      "metadata": {
        "id": "heIT25XIpRc2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# attempt to autodetect the device\n",
        "import time\n",
        "torch.backends.cuda.matmul.allow_tf32 = True  # no effect on T4, safe to enable\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "torch.set_float32_matmul_precision(\"high\")\n",
        "\n",
        "\n",
        "device = \"cpu\"\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "print(f\"using device: {device}\")\n",
        "# device = \"cpu\" #OVERRIDE\n",
        "# max_length = 30\n",
        "# num_return_sequences = 5\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(1337)\n",
        "\n",
        "train_loader = DataLoaderLite(B=8, T=1024)\n",
        "\n",
        "\n",
        "# get the logits\n",
        "model = GPT(GPTConfig(vocab_size=50304))\n",
        "# model = GPT.from_pretrained(\"gpt2\")\n",
        "model.to(device)\n",
        "\n",
        "#compile the model\n",
        "model = torch.compile(model)\n",
        "\n",
        "#optimize!\n",
        "from torch import amp\n",
        "\n",
        "scaler = amp.GradScaler(device=\"cuda\")  # NEW: enable mixed precision\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, betas=(0.9, 0.95), eps=1e-8)\n",
        "\n",
        "for i in range(50):\n",
        "    t0 = time.time()\n",
        "\n",
        "    x, y = train_loader.next_batch()\n",
        "    x, y = x.to(device), y.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # FP16 forward + loss (autocast enables mixed precision on T4)\n",
        "    with amp.autocast(\"cuda\"):\n",
        "        logits, loss = model(x, y)\n",
        "\n",
        "    # backward pass (scaled for safe gradients in FP16)\n",
        "    scaler.scale(loss).backward()\n",
        "\n",
        "    #unscale before clipping\n",
        "    scaler.unscale_(optimizer)\n",
        "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "    # update weights\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    t1 = time.time()\n",
        "\n",
        "    dt = (t1 - t0) * 100  # time difference in milliseconds\n",
        "    tokens_per_sec = (train_loader.B * train_loader.T) / (t1 - t0)\n",
        "    print(f\" step {i:4d} | loss: {loss.item():.6f} | norm: {norm:.4f} | dt: {dt:.2f}ms | tok/sec: {tokens_per_sec:.2f}\")\n",
        "\n",
        "import sys; sys.exit(0)\n",
        "\n",
        "\n",
        "#prefix tokens\n",
        "import tiktoken\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "tokens = enc.encode(\"Hello, I'm a language model,\")\n",
        "tokens = torch.tensor(tokens, dtype=torch.long) #(8, )\n",
        "tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1) #(5, 8)\n",
        "x = tokens.to(device)\n",
        "\n",
        "\n",
        "# generate! right now x is (B, T) where B = 5, T = 8\n",
        "# set the seed is 42 (my fav what is life :P)\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "while x.size(1) < max_length:\n",
        "    # forward the model to get the logits\n",
        "    with torch.no_grad():\n",
        "        logits = model(x) # (B, T, vocab_size)\n",
        "        # take the logits at the last position\n",
        "        logits = logits[:, -1, :] # (B, vocab_size)\n",
        "        # get the probabilities\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        # do top-k sampling of 50 (huggingface pipeline default)\n",
        "        # topk_probs here becomes (5, 50), topk_indices is  (5, 50)\n",
        "        topk_probs, topk_indices = torch.topk(probs, k=50, dim=-1)\n",
        "        # select a token from the top-k probabilities\n",
        "        ix = torch.multinomial(topk_probs, num_samples=1) # (B, 1)\n",
        "        # gather the corresponding indices\n",
        "        xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
        "        # append to the sequnce\n",
        "        x = torch.cat((x, xcol), dim=1)\n",
        "\n",
        "# print the generated text\n",
        "for i in range(num_return_sequences):\n",
        "    tokens = x[i, :max_length].tolist()\n",
        "    decoded = enc.decode(tokens)\n",
        "    print(\">\", decoded)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KDh-s5XznoTm",
        "outputId": "ca3bf30f-7889-4516-d4c2-40ee73a8cbf7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using device: cuda\n",
            "loaded 338025 tokens\n",
            "1 epoch = 41 batches\n",
            " step    0 | loss: 10.945843 | norm: 15.1871 | dt: 49.95ms | tok/sec: 16400.08\n",
            " step    1 | loss: 9.607196 | norm: 4.7175 | dt: 45.34ms | tok/sec: 18069.54\n",
            " step    2 | loss: 8.898447 | norm: 4.4026 | dt: 47.75ms | tok/sec: 17155.79\n",
            " step    3 | loss: 8.856247 | norm: 4.1560 | dt: 44.61ms | tok/sec: 18363.70\n",
            " step    4 | loss: 8.624682 | norm: 3.7552 | dt: 46.28ms | tok/sec: 17699.27\n",
            " step    5 | loss: 8.443286 | norm: 3.0065 | dt: 45.10ms | tok/sec: 18166.00\n",
            " step    6 | loss: 8.361417 | norm: 2.0013 | dt: 45.66ms | tok/sec: 17939.72\n",
            " step    7 | loss: 8.069830 | norm: 2.7407 | dt: 45.26ms | tok/sec: 18100.41\n",
            " step    8 | loss: 7.767623 | norm: 2.1683 | dt: 45.56ms | tok/sec: 17980.40\n",
            " step    9 | loss: 7.696859 | norm: 1.4392 | dt: 45.85ms | tok/sec: 17865.52\n",
            " step   10 | loss: 7.678771 | norm: 1.9711 | dt: 44.91ms | tok/sec: 18241.73\n",
            " step   11 | loss: 7.481083 | norm: 1.9221 | dt: 46.50ms | tok/sec: 17617.73\n",
            " step   12 | loss: 7.419626 | norm: 1.6536 | dt: 45.57ms | tok/sec: 17978.16\n",
            " step   13 | loss: 7.185840 | norm: 1.3559 | dt: 46.04ms | tok/sec: 17791.96\n",
            " step   14 | loss: 7.056791 | norm: 1.1864 | dt: 46.36ms | tok/sec: 17670.74\n",
            " step   15 | loss: 6.863747 | norm: 1.1622 | dt: 46.35ms | tok/sec: 17674.69\n",
            " step   16 | loss: 6.765328 | norm: 1.0106 | dt: 46.63ms | tok/sec: 17567.33\n",
            " step   17 | loss: 6.741601 | norm: 0.9631 | dt: 46.68ms | tok/sec: 17551.08\n",
            " step   18 | loss: 6.606034 | norm: 0.8761 | dt: 46.69ms | tok/sec: 17544.55\n",
            " step   19 | loss: 6.433076 | norm: 1.1893 | dt: 46.66ms | tok/sec: 17558.64\n",
            " step   20 | loss: 6.495338 | norm: 0.8605 | dt: 46.66ms | tok/sec: 17557.04\n",
            " step   21 | loss: 6.369368 | norm: 1.9437 | dt: 46.42ms | tok/sec: 17646.60\n",
            " step   22 | loss: 6.461160 | norm: 0.9162 | dt: 46.38ms | tok/sec: 17662.72\n",
            " step   23 | loss: 6.331674 | norm: 1.2124 | dt: 47.39ms | tok/sec: 17286.16\n",
            " step   24 | loss: 6.326596 | norm: 0.9861 | dt: 46.80ms | tok/sec: 17505.46\n",
            " step   25 | loss: 6.384728 | norm: 0.9720 | dt: 47.46ms | tok/sec: 17260.57\n",
            " step   26 | loss: 6.594298 | norm: 1.0077 | dt: 46.96ms | tok/sec: 17443.11\n",
            " step   27 | loss: 6.464881 | norm: 1.3451 | dt: 46.78ms | tok/sec: 17513.24\n",
            " step   28 | loss: 6.712249 | norm: 1.0155 | dt: 46.73ms | tok/sec: 17529.41\n",
            " step   29 | loss: 6.471904 | norm: 0.8859 | dt: 48.11ms | tok/sec: 17028.71\n",
            " step   30 | loss: 6.454643 | norm: 0.9874 | dt: 47.38ms | tok/sec: 17290.97\n",
            " step   31 | loss: 6.435269 | norm: 1.3908 | dt: 47.93ms | tok/sec: 17091.84\n",
            " step   32 | loss: 6.273798 | norm: 1.1862 | dt: 48.14ms | tok/sec: 17015.27\n",
            " step   33 | loss: 6.591605 | norm: 1.4044 | dt: 47.24ms | tok/sec: 17342.38\n",
            " step   34 | loss: 6.456623 | norm: 1.1472 | dt: 48.84ms | tok/sec: 16771.86\n",
            " step   35 | loss: 6.350178 | norm: 1.1826 | dt: 48.49ms | tok/sec: 16894.56\n",
            " step   36 | loss: 6.388961 | norm: 1.1019 | dt: 48.87ms | tok/sec: 16762.02\n",
            " step   37 | loss: 6.409139 | norm: 0.9388 | dt: 48.50ms | tok/sec: 16889.12\n",
            " step   38 | loss: 6.164969 | norm: 0.9555 | dt: 48.85ms | tok/sec: 16769.59\n",
            " step   39 | loss: 6.199170 | norm: 1.0550 | dt: 48.47ms | tok/sec: 16899.73\n",
            " step   40 | loss: 6.480654 | norm: 1.2311 | dt: 48.71ms | tok/sec: 16816.41\n",
            " step   41 | loss: 6.343282 | norm: 1.0469 | dt: 48.39ms | tok/sec: 16929.33\n",
            " step   42 | loss: 6.371363 | norm: 1.1653 | dt: 48.59ms | tok/sec: 16859.58\n",
            " step   43 | loss: 6.081771 | norm: 1.1691 | dt: 48.73ms | tok/sec: 16812.01\n",
            " step   44 | loss: 5.995671 | norm: 0.9699 | dt: 48.95ms | tok/sec: 16735.20\n",
            " step   45 | loss: 6.055957 | norm: 1.0340 | dt: 48.53ms | tok/sec: 16880.20\n",
            " step   46 | loss: 6.168617 | norm: 0.9318 | dt: 48.44ms | tok/sec: 16910.23\n",
            " step   47 | loss: 6.152355 | norm: 1.3933 | dt: 48.80ms | tok/sec: 16786.08\n",
            " step   48 | loss: 6.023395 | norm: 1.0882 | dt: 48.68ms | tok/sec: 16829.94\n",
            " step   49 | loss: 5.891482 | norm: 1.1263 | dt: 48.95ms | tok/sec: 16735.78\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "0",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vocab size = 50304(nice number!!)  + Flash Attention + FP16 + grad scaler + AMP, batch_size=8, time:41ms"
      ],
      "metadata": {
        "id": "iPEiKxowmjDa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# attempt to autodetect the device\n",
        "import time\n",
        "torch.backends.cuda.matmul.allow_tf32 = True  # no effect on T4, safe to enable\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "torch.set_float32_matmul_precision(\"high\")\n",
        "\n",
        "\n",
        "device = \"cpu\"\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "print(f\"using device: {device}\")\n",
        "# device = \"cpu\" #OVERRIDE\n",
        "# max_length = 30\n",
        "# num_return_sequences = 5\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(1337)\n",
        "\n",
        "train_loader = DataLoaderLite(B=8, T=1024)\n",
        "\n",
        "\n",
        "# get the logits\n",
        "model = GPT(GPTConfig(vocab_size=50304))\n",
        "# model = GPT.from_pretrained(\"gpt2\")\n",
        "model.to(device)\n",
        "\n",
        "#compile the model\n",
        "model = torch.compile(model)\n",
        "\n",
        "#optimize!\n",
        "from torch import amp\n",
        "\n",
        "scaler = amp.GradScaler(device=\"cuda\")  # NEW: enable mixed precision\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
        "\n",
        "for i in range(50):\n",
        "    t0 = time.time()\n",
        "\n",
        "    x, y = train_loader.next_batch()\n",
        "    x, y = x.to(device), y.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # FP16 forward + loss (autocast enables mixed precision on T4)\n",
        "    with amp.autocast(\"cuda\"):\n",
        "        logits, loss = model(x, y)\n",
        "\n",
        "    # backward pass (scaled for safe gradients in FP16)\n",
        "    scaler.scale(loss).backward()\n",
        "\n",
        "    # update weights\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    t1 = time.time()\n",
        "\n",
        "    dt = (t1 - t0) * 100  # time difference in milliseconds\n",
        "    tokens_per_sec = (train_loader.B * train_loader.T) / (t1 - t0)\n",
        "    print(f\" step {i}, loss: {loss.item()}, dt: {dt:.2f}ms, tok/sec: {tokens_per_sec:.2f}\")\n",
        "\n",
        "import sys; sys.exit(0)\n",
        "\n",
        "\n",
        "#prefix tokens\n",
        "import tiktoken\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "tokens = enc.encode(\"Hello, I'm a language model,\")\n",
        "tokens = torch.tensor(tokens, dtype=torch.long) #(8, )\n",
        "tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1) #(5, 8)\n",
        "x = tokens.to(device)\n",
        "\n",
        "\n",
        "# generate! right now x is (B, T) where B = 5, T = 8\n",
        "# set the seed is 42 (my fav what is life :P)\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "while x.size(1) < max_length:\n",
        "    # forward the model to get the logits\n",
        "    with torch.no_grad():\n",
        "        logits = model(x) # (B, T, vocab_size)\n",
        "        # take the logits at the last position\n",
        "        logits = logits[:, -1, :] # (B, vocab_size)\n",
        "        # get the probabilities\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        # do top-k sampling of 50 (huggingface pipeline default)\n",
        "        # topk_probs here becomes (5, 50), topk_indices is  (5, 50)\n",
        "        topk_probs, topk_indices = torch.topk(probs, k=50, dim=-1)\n",
        "        # select a token from the top-k probabilities\n",
        "        ix = torch.multinomial(topk_probs, num_samples=1) # (B, 1)\n",
        "        # gather the corresponding indices\n",
        "        xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
        "        # append to the sequnce\n",
        "        x = torch.cat((x, xcol), dim=1)\n",
        "\n",
        "# print the generated text\n",
        "for i in range(num_return_sequences):\n",
        "    tokens = x[i, :max_length].tolist()\n",
        "    decoded = enc.decode(tokens)\n",
        "    print(\">\", decoded)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zOWmh_UjmUai",
        "outputId": "8a8f917c-bddd-4a76-8f2e-64128e8c4d71"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using device: cuda\n",
            "loaded 338025 tokens\n",
            "1 epoch = 41 batches\n",
            " step 0, loss: 10.945842742919922, dt: 49.25ms, tok/sec: 16633.03\n",
            " step 1, loss: 9.606761932373047, dt: 43.78ms, tok/sec: 18712.09\n",
            " step 2, loss: 8.932099342346191, dt: 45.56ms, tok/sec: 17981.86\n",
            " step 3, loss: 8.75814151763916, dt: 43.63ms, tok/sec: 18776.98\n",
            " step 4, loss: 8.625509262084961, dt: 44.42ms, tok/sec: 18440.69\n",
            " step 5, loss: 8.481952667236328, dt: 43.89ms, tok/sec: 18666.34\n",
            " step 6, loss: 8.383893966674805, dt: 43.87ms, tok/sec: 18671.73\n",
            " step 7, loss: 8.065706253051758, dt: 44.32ms, tok/sec: 18484.42\n",
            " step 8, loss: 7.807280540466309, dt: 43.96ms, tok/sec: 18636.41\n",
            " step 9, loss: 7.785613059997559, dt: 43.69ms, tok/sec: 18751.84\n",
            " step 10, loss: 7.750455856323242, dt: 43.48ms, tok/sec: 18841.01\n",
            " step 11, loss: 7.555953025817871, dt: 43.11ms, tok/sec: 19003.71\n",
            " step 12, loss: 7.504838943481445, dt: 43.99ms, tok/sec: 18624.47\n",
            " step 13, loss: 7.300006866455078, dt: 42.99ms, tok/sec: 19053.48\n",
            " step 14, loss: 7.192455291748047, dt: 44.20ms, tok/sec: 18535.03\n",
            " step 15, loss: 7.030913352966309, dt: 42.63ms, tok/sec: 19218.66\n",
            " step 16, loss: 6.92523193359375, dt: 43.88ms, tok/sec: 18667.44\n",
            " step 17, loss: 6.915282249450684, dt: 43.22ms, tok/sec: 18953.93\n",
            " step 18, loss: 6.785163402557373, dt: 43.99ms, tok/sec: 18620.90\n",
            " step 19, loss: 6.652517318725586, dt: 43.72ms, tok/sec: 18735.59\n",
            " step 20, loss: 6.690572738647461, dt: 43.77ms, tok/sec: 18716.40\n",
            " step 21, loss: 6.565331935882568, dt: 44.10ms, tok/sec: 18575.40\n",
            " step 22, loss: 6.647122859954834, dt: 43.15ms, tok/sec: 18986.53\n",
            " step 23, loss: 6.525320529937744, dt: 44.14ms, tok/sec: 18558.07\n",
            " step 24, loss: 6.506521701812744, dt: 43.58ms, tok/sec: 18797.45\n",
            " step 25, loss: 6.568511962890625, dt: 43.91ms, tok/sec: 18654.25\n",
            " step 26, loss: 6.6771240234375, dt: 44.35ms, tok/sec: 18470.39\n",
            " step 27, loss: 6.573500156402588, dt: 44.54ms, tok/sec: 18391.11\n",
            " step 28, loss: 6.781764984130859, dt: 44.62ms, tok/sec: 18361.29\n",
            " step 29, loss: 6.615392208099365, dt: 44.40ms, tok/sec: 18449.97\n",
            " step 30, loss: 6.596870422363281, dt: 44.56ms, tok/sec: 18385.49\n",
            " step 31, loss: 6.6067328453063965, dt: 44.52ms, tok/sec: 18399.91\n",
            " step 32, loss: 6.457540512084961, dt: 44.51ms, tok/sec: 18404.88\n",
            " step 33, loss: 6.766098976135254, dt: 44.88ms, tok/sec: 18251.86\n",
            " step 34, loss: 6.6360273361206055, dt: 44.60ms, tok/sec: 18365.79\n",
            " step 35, loss: 6.525035858154297, dt: 44.98ms, tok/sec: 18213.77\n",
            " step 36, loss: 6.572842121124268, dt: 45.71ms, tok/sec: 17920.28\n",
            " step 37, loss: 6.577075958251953, dt: 44.84ms, tok/sec: 18268.33\n",
            " step 38, loss: 6.494317054748535, dt: 46.18ms, tok/sec: 17741.12\n",
            " step 39, loss: 6.41579532623291, dt: 45.02ms, tok/sec: 18195.01\n",
            " step 40, loss: 6.635256767272949, dt: 44.90ms, tok/sec: 18246.16\n",
            " step 41, loss: 6.380028247833252, dt: 46.13ms, tok/sec: 17759.29\n",
            " step 42, loss: 6.4346818923950195, dt: 45.56ms, tok/sec: 17982.40\n",
            " step 43, loss: 6.176999092102051, dt: 45.71ms, tok/sec: 17923.38\n",
            " step 44, loss: 6.119990825653076, dt: 45.94ms, tok/sec: 17831.51\n",
            " step 45, loss: 6.187306880950928, dt: 46.04ms, tok/sec: 17794.46\n",
            " step 46, loss: 6.317660331726074, dt: 45.56ms, tok/sec: 17980.62\n",
            " step 47, loss: 6.282604694366455, dt: 44.58ms, tok/sec: 18376.39\n",
            " step 48, loss: 6.176722049713135, dt: 46.72ms, tok/sec: 17534.20\n",
            " step 49, loss: 6.056689739227295, dt: 46.12ms, tok/sec: 17760.95\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "0",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Flash Attention + FP16 + grad scaler + AMP"
      ],
      "metadata": {
        "id": "1ePWjWCbmAYm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# attempt to autodetect the device\n",
        "import time\n",
        "torch.backends.cuda.matmul.allow_tf32 = True  # no effect on T4, safe to enable\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "torch.set_float32_matmul_precision(\"high\")\n",
        "\n",
        "\n",
        "device = \"cpu\"\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "print(f\"using device: {device}\")\n",
        "# device = \"cpu\" #OVERRIDE\n",
        "# max_length = 30\n",
        "# num_return_sequences = 5\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(1337)\n",
        "\n",
        "train_loader = DataLoaderLite(B=8, T=1024)\n",
        "\n",
        "\n",
        "# get the logits\n",
        "model = GPT(GPTConfig())\n",
        "# model = GPT.from_pretrained(\"gpt2\")\n",
        "model.to(device)\n",
        "\n",
        "#compile the model\n",
        "model = torch.compile(model)\n",
        "\n",
        "#optimize!\n",
        "from torch import amp\n",
        "\n",
        "scaler = amp.GradScaler(device=\"cuda\")  # NEW: enable mixed precision\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
        "\n",
        "for i in range(50):\n",
        "    t0 = time.time()\n",
        "\n",
        "    x, y = train_loader.next_batch()\n",
        "    x, y = x.to(device), y.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # FP16 forward + loss (autocast enables mixed precision on T4)\n",
        "    with amp.autocast(\"cuda\"):\n",
        "        logits, loss = model(x, y)\n",
        "\n",
        "    # backward pass (scaled for safe gradients in FP16)\n",
        "    scaler.scale(loss).backward()\n",
        "\n",
        "    # update weights\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    t1 = time.time()\n",
        "\n",
        "    dt = (t1 - t0) * 100  # time difference in milliseconds\n",
        "    tokens_per_sec = (train_loader.B * train_loader.T) / (t1 - t0)\n",
        "    print(f\" step {i}, loss: {loss.item()}, dt: {dt:.2f}ms, tok/sec: {tokens_per_sec:.2f}\")\n",
        "\n",
        "import sys; sys.exit(0)\n",
        "\n",
        "\n",
        "#prefix tokens\n",
        "import tiktoken\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "tokens = enc.encode(\"Hello, I'm a language model,\")\n",
        "tokens = torch.tensor(tokens, dtype=torch.long) #(8, )\n",
        "tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1) #(5, 8)\n",
        "x = tokens.to(device)\n",
        "\n",
        "\n",
        "# generate! right now x is (B, T) where B = 5, T = 8\n",
        "# set the seed is 42 (my fav what is life :P)\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "while x.size(1) < max_length:\n",
        "    # forward the model to get the logits\n",
        "    with torch.no_grad():\n",
        "        logits = model(x) # (B, T, vocab_size)\n",
        "        # take the logits at the last position\n",
        "        logits = logits[:, -1, :] # (B, vocab_size)\n",
        "        # get the probabilities\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        # do top-k sampling of 50 (huggingface pipeline default)\n",
        "        # topk_probs here becomes (5, 50), topk_indices is  (5, 50)\n",
        "        topk_probs, topk_indices = torch.topk(probs, k=50, dim=-1)\n",
        "        # select a token from the top-k probabilities\n",
        "        ix = torch.multinomial(topk_probs, num_samples=1) # (B, 1)\n",
        "        # gather the corresponding indices\n",
        "        xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
        "        # append to the sequnce\n",
        "        x = torch.cat((x, xcol), dim=1)\n",
        "\n",
        "# print the generated text\n",
        "for i in range(num_return_sequences):\n",
        "    tokens = x[i, :max_length].tolist()\n",
        "    decoded = enc.decode(tokens)\n",
        "    print(\">\", decoded)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZUiOiXRMk1wP",
        "outputId": "479047d3-1255-4ed0-e460-6119ae97fd3e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using device: cuda\n",
            "loaded 338025 tokens\n",
            "1 epoch = 41 batches\n",
            " step 0, loss: 10.914106369018555, dt: 3179.67ms, tok/sec: 257.64\n",
            " step 1, loss: 9.548101425170898, dt: 47.07ms, tok/sec: 17402.41\n",
            " step 2, loss: 8.843807220458984, dt: 44.54ms, tok/sec: 18391.02\n",
            " step 3, loss: 8.736413955688477, dt: 43.03ms, tok/sec: 19038.46\n",
            " step 4, loss: 8.498735427856445, dt: 43.59ms, tok/sec: 18792.44\n",
            " step 5, loss: 8.36872673034668, dt: 43.57ms, tok/sec: 18802.49\n",
            " step 6, loss: 8.34364128112793, dt: 43.35ms, tok/sec: 18895.53\n",
            " step 7, loss: 8.049447059631348, dt: 43.80ms, tok/sec: 18702.10\n",
            " step 8, loss: 7.739285469055176, dt: 42.89ms, tok/sec: 19098.87\n",
            " step 9, loss: 7.676781177520752, dt: 44.46ms, tok/sec: 18427.19\n",
            " step 10, loss: 7.6713643074035645, dt: 43.55ms, tok/sec: 18809.93\n",
            " step 11, loss: 7.512412071228027, dt: 44.49ms, tok/sec: 18411.07\n",
            " step 12, loss: 7.449471473693848, dt: 43.36ms, tok/sec: 18894.11\n",
            " step 13, loss: 7.239461898803711, dt: 43.42ms, tok/sec: 18867.24\n",
            " step 14, loss: 7.130849838256836, dt: 43.92ms, tok/sec: 18649.98\n",
            " step 15, loss: 6.966947078704834, dt: 43.62ms, tok/sec: 18779.74\n",
            " step 16, loss: 6.881011009216309, dt: 44.34ms, tok/sec: 18476.28\n",
            " step 17, loss: 6.884006500244141, dt: 43.83ms, tok/sec: 18690.49\n",
            " step 18, loss: 6.757608890533447, dt: 43.22ms, tok/sec: 18954.57\n",
            " step 19, loss: 6.617766380310059, dt: 42.85ms, tok/sec: 19115.77\n",
            " step 20, loss: 6.668762683868408, dt: 42.38ms, tok/sec: 19328.10\n",
            " step 21, loss: 6.566357612609863, dt: 43.35ms, tok/sec: 18898.15\n",
            " step 22, loss: 6.635318756103516, dt: 43.24ms, tok/sec: 18945.44\n",
            " step 23, loss: 6.5366597175598145, dt: 42.98ms, tok/sec: 19061.28\n",
            " step 24, loss: 6.5177741050720215, dt: 43.27ms, tok/sec: 18931.25\n",
            " step 25, loss: 6.576813697814941, dt: 43.65ms, tok/sec: 18768.75\n",
            " step 26, loss: 6.681463718414307, dt: 43.33ms, tok/sec: 18906.36\n",
            " step 27, loss: 6.579545974731445, dt: 43.89ms, tok/sec: 18664.09\n",
            " step 28, loss: 6.801132678985596, dt: 43.82ms, tok/sec: 18694.16\n",
            " step 29, loss: 6.6139631271362305, dt: 43.33ms, tok/sec: 18907.60\n",
            " step 30, loss: 6.593467712402344, dt: 43.73ms, tok/sec: 18735.06\n",
            " step 31, loss: 6.611301422119141, dt: 43.31ms, tok/sec: 18913.91\n",
            " step 32, loss: 6.46739387512207, dt: 44.27ms, tok/sec: 18503.03\n",
            " step 33, loss: 6.77272367477417, dt: 44.52ms, tok/sec: 18401.94\n",
            " step 34, loss: 6.633965492248535, dt: 44.17ms, tok/sec: 18546.67\n",
            " step 35, loss: 6.51342248916626, dt: 44.18ms, tok/sec: 18540.74\n",
            " step 36, loss: 6.686058044433594, dt: 44.30ms, tok/sec: 18490.45\n",
            " step 37, loss: 6.605645179748535, dt: 44.13ms, tok/sec: 18563.97\n",
            " step 38, loss: 6.398375511169434, dt: 44.21ms, tok/sec: 18529.95\n",
            " step 39, loss: 6.459415435791016, dt: 44.36ms, tok/sec: 18466.97\n",
            " step 40, loss: 6.631990432739258, dt: 44.51ms, tok/sec: 18406.73\n",
            " step 41, loss: 6.395315170288086, dt: 44.75ms, tok/sec: 18305.93\n",
            " step 42, loss: 6.436728477478027, dt: 44.30ms, tok/sec: 18491.77\n",
            " step 43, loss: 6.182994842529297, dt: 44.49ms, tok/sec: 18411.99\n",
            " step 44, loss: 6.125860214233398, dt: 44.82ms, tok/sec: 18275.60\n",
            " step 45, loss: 6.2152814865112305, dt: 44.54ms, tok/sec: 18392.53\n",
            " step 46, loss: 6.327800750732422, dt: 44.68ms, tok/sec: 18334.56\n",
            " step 47, loss: 6.274053573608398, dt: 44.37ms, tok/sec: 18464.44\n",
            " step 48, loss: 6.187404632568359, dt: 44.60ms, tok/sec: 18367.17\n",
            " step 49, loss: 6.060297012329102, dt: 44.23ms, tok/sec: 18522.78\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "0",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FP16 + grad scaler + AMP"
      ],
      "metadata": {
        "id": "nynnHBzhl4Yo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# attempt to autodetect the device\n",
        "import time\n",
        "torch.backends.cuda.matmul.allow_tf32 = True  # no effect on T4, safe to enable\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "torch.set_float32_matmul_precision(\"high\")\n",
        "\n",
        "\n",
        "device = \"cpu\"\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "print(f\"using device: {device}\")\n",
        "# device = \"cpu\" #OVERRIDE\n",
        "# max_length = 30\n",
        "# num_return_sequences = 5\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(1337)\n",
        "\n",
        "train_loader = DataLoaderLite(B=8, T=1024)\n",
        "\n",
        "\n",
        "# get the logits\n",
        "model = GPT(GPTConfig())\n",
        "# model = GPT.from_pretrained(\"gpt2\")\n",
        "model.to(device)\n",
        "\n",
        "#compile the model\n",
        "model = torch.compile(model)\n",
        "\n",
        "#optimize!\n",
        "from torch import amp\n",
        "\n",
        "scaler = amp.GradScaler(device=\"cuda\")  # NEW: enable mixed precision\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
        "\n",
        "for i in range(50):\n",
        "    t0 = time.time()\n",
        "\n",
        "    x, y = train_loader.next_batch()\n",
        "    x, y = x.to(device), y.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # FP16 forward + loss (autocast enables mixed precision on T4)\n",
        "    with amp.autocast(\"cuda\"):\n",
        "        logits, loss = model(x, y)\n",
        "\n",
        "    # backward pass (scaled for safe gradients in FP16)\n",
        "    scaler.scale(loss).backward()\n",
        "\n",
        "    # update weights\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    t1 = time.time()\n",
        "\n",
        "    dt = (t1 - t0) * 100  # time difference in milliseconds\n",
        "    tokens_per_sec = (train_loader.B * train_loader.T) / (t1 - t0)\n",
        "    print(f\" step {i}, loss: {loss.item()}, dt: {dt:.2f}ms, tok/sec: {tokens_per_sec:.2f}\")\n",
        "\n",
        "import sys; sys.exit(0)\n",
        "\n",
        "\n",
        "#prefix tokens\n",
        "import tiktoken\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "tokens = enc.encode(\"Hello, I'm a language model,\")\n",
        "tokens = torch.tensor(tokens, dtype=torch.long) #(8, )\n",
        "tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1) #(5, 8)\n",
        "x = tokens.to(device)\n",
        "\n",
        "\n",
        "# generate! right now x is (B, T) where B = 5, T = 8\n",
        "# set the seed is 42 (my fav what is life :P)\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "while x.size(1) < max_length:\n",
        "    # forward the model to get the logits\n",
        "    with torch.no_grad():\n",
        "        logits = model(x) # (B, T, vocab_size)\n",
        "        # take the logits at the last position\n",
        "        logits = logits[:, -1, :] # (B, vocab_size)\n",
        "        # get the probabilities\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        # do top-k sampling of 50 (huggingface pipeline default)\n",
        "        # topk_probs here becomes (5, 50), topk_indices is  (5, 50)\n",
        "        topk_probs, topk_indices = torch.topk(probs, k=50, dim=-1)\n",
        "        # select a token from the top-k probabilities\n",
        "        ix = torch.multinomial(topk_probs, num_samples=1) # (B, 1)\n",
        "        # gather the corresponding indices\n",
        "        xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
        "        # append to the sequnce\n",
        "        x = torch.cat((x, xcol), dim=1)\n",
        "\n",
        "# print the generated text\n",
        "for i in range(num_return_sequences):\n",
        "    tokens = x[i, :max_length].tolist()\n",
        "    decoded = enc.decode(tokens)\n",
        "    print(\">\", decoded)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9S5uFXVQO0T0",
        "outputId": "0eee1af5-64a1-4c76-fba2-262a67307cca"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using device: cuda\n",
            "loaded 338025 tokens\n",
            "1 epoch = 41 batches\n",
            " step 0, loss: 10.914105415344238, dt: 5613.79ms, tok/sec: 145.93\n",
            " step 1, loss: 9.548095703125, dt: 58.02ms, tok/sec: 14119.42\n",
            " step 2, loss: 8.84383773803711, dt: 55.70ms, tok/sec: 14707.44\n",
            " step 3, loss: 8.736400604248047, dt: 54.90ms, tok/sec: 14922.54\n",
            " step 4, loss: 8.498760223388672, dt: 54.49ms, tok/sec: 15033.17\n",
            " step 5, loss: 8.368730545043945, dt: 54.69ms, tok/sec: 14979.76\n",
            " step 6, loss: 8.343643188476562, dt: 54.67ms, tok/sec: 14984.44\n",
            " step 7, loss: 8.049449920654297, dt: 54.55ms, tok/sec: 15016.63\n",
            " step 8, loss: 7.739282608032227, dt: 54.65ms, tok/sec: 14991.04\n",
            " step 9, loss: 7.67677116394043, dt: 54.70ms, tok/sec: 14977.12\n",
            " step 10, loss: 7.671354293823242, dt: 54.72ms, tok/sec: 14971.85\n",
            " step 11, loss: 7.512383460998535, dt: 54.59ms, tok/sec: 15007.05\n",
            " step 12, loss: 7.449495792388916, dt: 54.70ms, tok/sec: 14976.69\n",
            " step 13, loss: 7.239490509033203, dt: 54.74ms, tok/sec: 14964.50\n",
            " step 14, loss: 7.130850315093994, dt: 54.50ms, tok/sec: 15029.93\n",
            " step 15, loss: 6.966947555541992, dt: 54.63ms, tok/sec: 14994.08\n",
            " step 16, loss: 6.881012916564941, dt: 54.60ms, tok/sec: 15003.79\n",
            " step 17, loss: 6.883999824523926, dt: 54.58ms, tok/sec: 15008.00\n",
            " step 18, loss: 6.757607460021973, dt: 54.53ms, tok/sec: 15022.67\n",
            " step 19, loss: 6.617749214172363, dt: 54.63ms, tok/sec: 14996.26\n",
            " step 20, loss: 6.66876220703125, dt: 54.53ms, tok/sec: 15023.24\n",
            " step 21, loss: 6.566343307495117, dt: 54.61ms, tok/sec: 15001.64\n",
            " step 22, loss: 6.63530969619751, dt: 54.59ms, tok/sec: 15006.49\n",
            " step 23, loss: 6.536671161651611, dt: 54.73ms, tok/sec: 14966.89\n",
            " step 24, loss: 6.517783164978027, dt: 54.54ms, tok/sec: 15021.00\n",
            " step 25, loss: 6.576822280883789, dt: 54.75ms, tok/sec: 14963.08\n",
            " step 26, loss: 6.681469440460205, dt: 54.94ms, tok/sec: 14909.87\n",
            " step 27, loss: 6.579545974731445, dt: 54.74ms, tok/sec: 14964.07\n",
            " step 28, loss: 6.801139831542969, dt: 54.52ms, tok/sec: 15026.35\n",
            " step 29, loss: 6.61398983001709, dt: 54.69ms, tok/sec: 14980.24\n",
            " step 30, loss: 6.593478679656982, dt: 54.60ms, tok/sec: 15004.39\n",
            " step 31, loss: 6.611293792724609, dt: 54.69ms, tok/sec: 14979.60\n",
            " step 32, loss: 6.467401504516602, dt: 54.61ms, tok/sec: 15001.55\n",
            " step 33, loss: 6.772706031799316, dt: 54.67ms, tok/sec: 14984.56\n",
            " step 34, loss: 6.633979320526123, dt: 54.59ms, tok/sec: 15005.87\n",
            " step 35, loss: 6.513415813446045, dt: 54.77ms, tok/sec: 14958.10\n",
            " step 36, loss: 6.685343265533447, dt: 56.76ms, tok/sec: 14432.50\n",
            " step 37, loss: 6.605646133422852, dt: 56.43ms, tok/sec: 14515.87\n",
            " step 38, loss: 6.398370265960693, dt: 56.17ms, tok/sec: 14583.86\n",
            " step 39, loss: 6.459357261657715, dt: 55.02ms, tok/sec: 14889.07\n",
            " step 40, loss: 6.631955146789551, dt: 56.61ms, tok/sec: 14472.08\n",
            " step 41, loss: 6.395318031311035, dt: 54.80ms, tok/sec: 14948.08\n",
            " step 42, loss: 6.436789035797119, dt: 56.52ms, tok/sec: 14493.95\n",
            " step 43, loss: 6.183072090148926, dt: 56.52ms, tok/sec: 14495.01\n",
            " step 44, loss: 6.125939846038818, dt: 56.76ms, tok/sec: 14433.31\n",
            " step 45, loss: 6.215404510498047, dt: 56.60ms, tok/sec: 14472.95\n",
            " step 46, loss: 6.327884674072266, dt: 56.56ms, tok/sec: 14484.84\n",
            " step 47, loss: 6.274044990539551, dt: 56.62ms, tok/sec: 14467.92\n",
            " step 48, loss: 6.187492370605469, dt: 56.62ms, tok/sec: 14468.04\n",
            " step 49, loss: 6.06046199798584, dt: 56.69ms, tok/sec: 14450.25\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "0",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xCfTc82GAHB5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}